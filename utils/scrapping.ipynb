{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9218c2ce-deed-4053-aec3-954cfae82ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import time\n",
    "import logging\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from threading import Lock\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class ProgressCounter:\n",
    "    def __init__(self, total):\n",
    "        self.lock = Lock()\n",
    "        self.current = 0\n",
    "        self.total = total\n",
    "        self.success = 0\n",
    "        self.failed = 0\n",
    "    \n",
    "    def increment(self, success=True):\n",
    "        with self.lock:\n",
    "            self.current += 1\n",
    "            if success:\n",
    "                self.success += 1\n",
    "            else:\n",
    "                self.failed += 1\n",
    "            return self.current\n",
    "\n",
    "def normalize_name(name):\n",
    "    \"\"\"Normalize athlete name for URL formatting.\"\"\"\n",
    "    if pd.isna(name):\n",
    "        return None\n",
    "    \n",
    "    name = str(name).lower()\n",
    "    # Remove accents and special characters\n",
    "    name = re.sub(r'[^\\w\\s-]', '', name)\n",
    "    name = re.sub(r'\\s+', '-', name.strip())\n",
    "    name = re.sub(r'-+', '-', name)\n",
    "    return name\n",
    "\n",
    "def clean_image_url(url):\n",
    "    \"\"\"\n",
    "    Clean and validate image URL.\n",
    "    Replaces template placeholders and ensures URL is complete.\n",
    "    \"\"\"\n",
    "    if not url:\n",
    "        return None\n",
    "    \n",
    "    # Replace template placeholders\n",
    "    url = url.replace('{formatInstructions}', 't_1-1_300/f_auto')\n",
    "    url = url.replace('{format}', 't_1-1_300/f_auto')\n",
    "    url = url.replace('\\\\/', '/')\n",
    "    \n",
    "    # Check if URL is complete (should not end with incomplete template)\n",
    "    if '{' in url or url.endswith('/private/'):\n",
    "        logger.warning(f\"Incomplete URL detected: {url[:100]}\")\n",
    "        return None\n",
    "    \n",
    "    # Ensure it's a valid Olympics image URL\n",
    "    if 'img.olympics.com' not in url:\n",
    "        return None\n",
    "    \n",
    "    return url\n",
    "\n",
    "def get_name_variations(name):\n",
    "    \"\"\"\n",
    "    Generate different name format variations to try.\n",
    "    Handles cases like \"LASTNAME Firstname\" vs \"Firstname LASTNAME\"\n",
    "    \n",
    "    Returns list of normalized name variations to try.\n",
    "    \"\"\"\n",
    "    if pd.isna(name):\n",
    "        return []\n",
    "    \n",
    "    variations = []\n",
    "    name_parts = str(name).strip().split()\n",
    "    \n",
    "    if len(name_parts) >= 2:\n",
    "        # Original order: \"LASTNAME Firstname\" -> \"lastname-firstname\"\n",
    "        original = normalize_name(name)\n",
    "        if original:\n",
    "            variations.append(original)\n",
    "        \n",
    "        # Reversed order: \"Firstname LASTNAME\" -> \"firstname-lastname\"\n",
    "        reversed_name = ' '.join(reversed(name_parts))\n",
    "        reversed_normalized = normalize_name(reversed_name)\n",
    "        if reversed_normalized and reversed_normalized != original:\n",
    "            variations.append(reversed_normalized)\n",
    "        \n",
    "        # Try with just first and last name (skip middle names)\n",
    "        if len(name_parts) > 2:\n",
    "            # First + Last\n",
    "            first_last = f\"{name_parts[0]} {name_parts[-1]}\"\n",
    "            first_last_norm = normalize_name(first_last)\n",
    "            if first_last_norm and first_last_norm not in variations:\n",
    "                variations.append(first_last_norm)\n",
    "            \n",
    "            # Last + First\n",
    "            last_first = f\"{name_parts[-1]} {name_parts[0]}\"\n",
    "            last_first_norm = normalize_name(last_first)\n",
    "            if last_first_norm and last_first_norm not in variations:\n",
    "                variations.append(last_first_norm)\n",
    "    elif len(name_parts) == 1:\n",
    "        # Single name\n",
    "        normalized = normalize_name(name)\n",
    "        if normalized:\n",
    "            variations.append(normalized)\n",
    "    \n",
    "    return variations\n",
    "\n",
    "def extract_image_url_enhanced(html_content, athlete_name):\n",
    "    \"\"\"\n",
    "    Enhanced image URL extraction targeting athlete profile images specifically.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "        \n",
    "        # Strategy 1: Look for athlete-image div or similar athlete photo containers\n",
    "        athlete_containers = [\n",
    "            soup.find('div', class_=re.compile(r'athlete.*image', re.I)),\n",
    "            soup.find('div', class_=re.compile(r'profile.*image', re.I)),\n",
    "            soup.find('div', class_=re.compile(r'player.*image', re.I)),\n",
    "            soup.find('figure', class_=re.compile(r'athlete', re.I)),\n",
    "        ]\n",
    "        \n",
    "        for container in athlete_containers:\n",
    "            if container:\n",
    "                # Look for img tag in this container\n",
    "                img = container.find('img')\n",
    "                if img:\n",
    "                    src = img.get('src') or img.get('data-src') or img.get('data-lazy-src')\n",
    "                    if src and 'img.olympics.com' in src:\n",
    "                        # Clean up the URL\n",
    "                        src = src.replace('{formatInstructions}', 't_1-1_300/f_auto')\n",
    "                        src = src.replace('{format}', 't_1-1_300/f_auto')\n",
    "                        return src\n",
    "        \n",
    "        # Strategy 2: Look for Next.js JSON data - target athlete images specifically\n",
    "        scripts = soup.find_all('script', {'id': '__NEXT_DATA__'})\n",
    "        for script in scripts:\n",
    "            if script.string:\n",
    "                # Look for athlete-specific image patterns in JSON\n",
    "                patterns = [\n",
    "                    r'\"athlete\"[^}]*?\"imageUrl\":\"([^\"]+)\"',\n",
    "                    r'\"profile\"[^}]*?\"imageUrl\":\"([^\"]+)\"',\n",
    "                    r'\"headshot\"[^}]*?\"url\":\"([^\"]+)\"',\n",
    "                    r'\"photo\"[^}]*?\"url\":\"([^\"]+)\"',\n",
    "                ]\n",
    "                \n",
    "                for pattern in patterns:\n",
    "                    matches = re.findall(pattern, script.string, re.DOTALL)\n",
    "                    if matches:\n",
    "                        url = matches[0]\n",
    "                        url = url.replace('\\\\/', '/')\n",
    "                        url = url.replace('{formatInstructions}', 't_1-1_300/f_auto')\n",
    "                        url = url.replace('{format}', 't_1-1_300/f_auto')\n",
    "                        if 'img.olympics.com' in url:\n",
    "                            return url\n",
    "        \n",
    "        # Strategy 3: Look in __NEXT_DATA__ for any athlete image\n",
    "        for script in scripts:\n",
    "            if script.string:\n",
    "                # Find URLs that contain athlete-related paths\n",
    "                urls = re.findall(r'https://img\\.olympics\\.com/images/image/private/[^\"\\\\]+', script.string)\n",
    "                if urls:\n",
    "                    for url in urls:\n",
    "                        url = url.replace('\\\\/', '/')\n",
    "                        # Skip logos and icons\n",
    "                        if any(x in url.lower() for x in ['/logo', '/icon', '/flag', '/symbol']):\n",
    "                            continue\n",
    "                        # Prefer athlete/headshot URLs\n",
    "                        if any(x in url.lower() for x in ['athlete', 'headshot', 'profile', 'player']):\n",
    "                            url = url.replace('{formatInstructions}', 't_1-1_300/f_auto')\n",
    "                            url = url.replace('{format}', 't_1-1_300/f_auto')\n",
    "                            return url\n",
    "                    # If no specific athlete URL, return first non-logo image\n",
    "                    if urls:\n",
    "                        url = urls[0].replace('{formatInstructions}', 't_1-1_300/f_auto')\n",
    "                        url = url.replace('{format}', 't_1-1_300/f_auto')\n",
    "                        return url\n",
    "        \n",
    "        # Strategy 4: Meta tags (og:image often has athlete photo)\n",
    "        meta_tags = [\n",
    "            ('property', 'og:image'),\n",
    "            ('name', 'twitter:image'),\n",
    "        ]\n",
    "        \n",
    "        for attr, value in meta_tags:\n",
    "            tag = soup.find('meta', {attr: value})\n",
    "            if tag and tag.get('content'):\n",
    "                img_url = tag.get('content')\n",
    "                if 'img.olympics.com' in img_url and '/logo' not in img_url.lower():\n",
    "                    img_url = img_url.replace('{formatInstructions}', 't_1-1_300/f_auto')\n",
    "                    return img_url\n",
    "        \n",
    "        # Strategy 5: Direct img tags with athlete class\n",
    "        img_tags = soup.find_all('img', class_=re.compile(r'athlete|profile|headshot', re.I))\n",
    "        for img in img_tags:\n",
    "            src = img.get('src') or img.get('data-src')\n",
    "            if src and 'img.olympics.com' in src:\n",
    "                src = src.replace('{formatInstructions}', 't_1-1_300/f_auto')\n",
    "                return src\n",
    "        \n",
    "        return None\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error extracting image URL for {athlete_name}: {e}\")\n",
    "        return None\n",
    "\n",
    "def scrape_athlete_image(athlete_name, base_url=\"https://www.olympics.com/fr/athletes/\"):\n",
    "    \"\"\"\n",
    "    Scrape athlete image URL from Olympics website.\n",
    "    Tries multiple name format variations.\n",
    "    \"\"\"\n",
    "    if not athlete_name or pd.isna(athlete_name):\n",
    "        return None\n",
    "    \n",
    "    # Get all possible name variations\n",
    "    name_variations = get_name_variations(athlete_name)\n",
    "    \n",
    "    if not name_variations:\n",
    "        logger.warning(f\"Could not generate name variations for: {athlete_name}\")\n",
    "        return None\n",
    "    \n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',\n",
    "        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
    "        'Accept-Language': 'fr-FR,fr;q=0.9,en-US;q=0.8,en;q=0.7',\n",
    "        'Accept-Encoding': 'gzip, deflate, br',\n",
    "        'Connection': 'keep-alive',\n",
    "        'Upgrade-Insecure-Requests': '1',\n",
    "        'Sec-Fetch-Dest': 'document',\n",
    "        'Sec-Fetch-Mode': 'navigate',\n",
    "        'Sec-Fetch-Site': 'none',\n",
    "    }\n",
    "    \n",
    "    # Try each name variation\n",
    "    for i, normalized_name in enumerate(name_variations):\n",
    "        url = f\"{base_url}{normalized_name}\"\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(url, headers=headers, timeout=15)\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                image_url = extract_image_url_enhanced(response.text, athlete_name)\n",
    "                if image_url:\n",
    "                    # Clean and validate the URL\n",
    "                    image_url = clean_image_url(image_url)\n",
    "                    if image_url:\n",
    "                        if i > 0:  # Log if we found it with an alternative format\n",
    "                            logger.info(f\"  → Found using variation #{i+1}: {normalized_name}\")\n",
    "                        return image_url\n",
    "            elif response.status_code == 404:\n",
    "                logger.debug(f\"404 for variation: {normalized_name}\")\n",
    "            else:\n",
    "                logger.warning(f\"HTTP {response.status_code} for {normalized_name}\")\n",
    "            \n",
    "            # Small delay between attempts to be polite\n",
    "            if i < len(name_variations) - 1:\n",
    "                time.sleep(0.2)\n",
    "                \n",
    "        except requests.Timeout:\n",
    "            logger.warning(f\"Timeout for {normalized_name}\")\n",
    "            continue\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error for {normalized_name}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # If we tried all variations and found nothing\n",
    "    logger.debug(f\"No image found after trying {len(name_variations)} variations: {', '.join(name_variations)}\")\n",
    "    return None\n",
    "\n",
    "def debug_single_athlete(athlete_name, save_html=False):\n",
    "    \"\"\"\n",
    "    Debug helper to examine HTML structure for a single athlete.\n",
    "    Shows all name variations being tried and returns the found URL.\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"DEBUGGING: {athlete_name}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    name_variations = get_name_variations(athlete_name)\n",
    "    print(f\"Name variations to try ({len(name_variations)}):\")\n",
    "    for i, var in enumerate(name_variations, 1):\n",
    "        print(f\"  {i}. {var}\")\n",
    "    print()\n",
    "    \n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',\n",
    "    }\n",
    "    \n",
    "    for i, normalized_name in enumerate(name_variations, 1):\n",
    "        url = f\"https://www.olympics.com/fr/athletes/{normalized_name}\"\n",
    "        print(f\"Attempt {i}/{len(name_variations)}: {url}\")\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(url, headers=headers, timeout=15)\n",
    "            print(f\"  Status Code: {response.status_code}\")\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                soup = BeautifulSoup(response.text, 'html.parser')\n",
    "                \n",
    "                # Save HTML if requested (only for successful response)\n",
    "                if save_html:\n",
    "                    filename = f\"debug_{normalized_name}.html\"\n",
    "                    with open(filename, 'w', encoding='utf-8') as f:\n",
    "                        f.write(response.text)\n",
    "                    print(f\"  HTML saved to: {filename}\")\n",
    "                \n",
    "                # Look for athlete-image divs\n",
    "                athlete_divs = soup.find_all('div', class_=re.compile(r'athlete.*image', re.I))\n",
    "                if athlete_divs:\n",
    "                    print(f\"  Found {len(athlete_divs)} div(s) with 'athlete-image' class:\")\n",
    "                    for j, div in enumerate(athlete_divs[:3], 1):\n",
    "                        print(f\"    Div {j}: classes = {div.get('class')}\")\n",
    "                        img = div.find('img')\n",
    "                        if img:\n",
    "                            src = img.get('src') or img.get('data-src')\n",
    "                            print(f\"      → img src: {src[:80] if src else 'None'}...\")\n",
    "                \n",
    "                # Try to extract image\n",
    "                image_url = extract_image_url_enhanced(response.text, athlete_name)\n",
    "                \n",
    "                if image_url:\n",
    "                    # Clean and validate\n",
    "                    image_url = clean_image_url(image_url)\n",
    "                    if image_url:\n",
    "                        print(f\"  ✓ Image found: {image_url}\")\n",
    "                        print(f\"\\n{'='*60}\")\n",
    "                        print(f\"SUCCESS! Image found with variation #{i}\")\n",
    "                        print(f\"{'='*60}\\n\")\n",
    "                        return image_url  # Return the URL instead of just returning\n",
    "                    else:\n",
    "                        print(f\"  ⚠ Image URL found but incomplete/invalid\")\n",
    "                else:\n",
    "                    print(f\"  ✗ No image found in response\")\n",
    "                    \n",
    "                    # Search for any img.olympics.com URLs\n",
    "                    olympics_urls = re.findall(r'https://[^\"\\']*img\\.olympics\\.com[^\"\\']*', response.text)\n",
    "                    if olympics_urls:\n",
    "                        print(f\"  Found {len(olympics_urls)} Olympics image URLs:\")\n",
    "                        for j, u in enumerate(olympics_urls[:3], 1):\n",
    "                            print(f\"    {j}. {u[:70]}...\")\n",
    "            elif response.status_code == 404:\n",
    "                print(f\"  ✗ Page not found (404)\")\n",
    "            else:\n",
    "                print(f\"  ✗ Unexpected status\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"  ✗ Error: {e}\")\n",
    "        \n",
    "        print()  # Blank line between attempts\n",
    "    \n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"RESULT: No image found after trying all variations\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    return None  # Return None if no image found\n",
    "\n",
    "def scrape_single_athlete(args):\n",
    "    \"\"\"Wrapper function for thread pool execution.\"\"\"\n",
    "    idx, athlete_name, counter = args\n",
    "    current = counter.increment(success=False)\n",
    "    logger.info(f\"[{current}/{counter.total}] Processing: {athlete_name}\")\n",
    "    \n",
    "    image_url = scrape_athlete_image(athlete_name)\n",
    "    \n",
    "    if image_url:\n",
    "        counter.success += 1\n",
    "        counter.failed -= 1\n",
    "        logger.info(f\"✓ Found image for {athlete_name}\")\n",
    "    else:\n",
    "        logger.warning(f\"✗ No image found for {athlete_name}\")\n",
    "    \n",
    "    return idx, image_url\n",
    "\n",
    "def test_first_n_rows(input_file, name_column='name', n=10, max_workers=5):\n",
    "    \"\"\"Test the scraper on the first N rows.\"\"\"\n",
    "    logger.info(f\"Reading CSV file: {input_file}\")\n",
    "    \n",
    "    try:\n",
    "        df = pd.read_csv(input_file)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error reading CSV: {e}\")\n",
    "        return None\n",
    "    \n",
    "    if name_column not in df.columns:\n",
    "        logger.error(f\"Column '{name_column}' not found. Available: {df.columns.tolist()}\")\n",
    "        return None\n",
    "    \n",
    "    df_test = df.head(n).copy()\n",
    "    df_test['image_url'] = None\n",
    "    \n",
    "    logger.info(f\"\\n{'='*60}\")\n",
    "    logger.info(f\"TESTING WITH FIRST {n} ROWS\")\n",
    "    logger.info(f\"Using {max_workers} concurrent threads\")\n",
    "    logger.info(f\"{'='*60}\\n\")\n",
    "    \n",
    "    logger.info(\"Athletes to process:\")\n",
    "    for idx, name in enumerate(df_test[name_column], 1):\n",
    "        logger.info(f\"  {idx}. {name}\")\n",
    "    logger.info(\"\")\n",
    "    \n",
    "    counter = ProgressCounter(len(df_test))\n",
    "    \n",
    "    args_list = [\n",
    "        (idx, row[name_column], counter)\n",
    "        for idx, row in df_test.iterrows()\n",
    "    ]\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = {executor.submit(scrape_single_athlete, args): args for args in args_list}\n",
    "        \n",
    "        for future in as_completed(futures):\n",
    "            try:\n",
    "                idx, image_url = future.result()\n",
    "                df_test.loc[idx, 'image_url'] = image_url\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Thread execution error: {e}\")\n",
    "    \n",
    "    elapsed_time = time.time() - start_time\n",
    "    \n",
    "    logger.info(f\"\\n{'='*60}\")\n",
    "    logger.info(f\"TEST RESULTS:\")\n",
    "    logger.info(f\"{'='*60}\")\n",
    "    logger.info(f\"Total processed: {len(df_test)}\")\n",
    "    logger.info(f\"Images found: {counter.success} ({counter.success/len(df_test)*100:.1f}%)\")\n",
    "    logger.info(f\"Images missing: {counter.failed} ({counter.failed/len(df_test)*100:.1f}%)\")\n",
    "    logger.info(f\"Time elapsed: {elapsed_time:.2f} seconds\")\n",
    "    logger.info(f\"Average time per athlete: {elapsed_time/len(df_test):.2f} seconds\")\n",
    "    logger.info(f\"{'='*60}\\n\")\n",
    "    \n",
    "    logger.info(\"DETAILED RESULTS:\")\n",
    "    for idx, row in df_test.iterrows():\n",
    "        status = \"✓\" if pd.notna(row['image_url']) else \"✗\"\n",
    "        url_preview = row['image_url'] if pd.notna(row['image_url']) else \"N/A\"\n",
    "        logger.info(f\"{status} {row[name_column]}: {url_preview}\")\n",
    "    \n",
    "    return df_test\n",
    "\n",
    "def process_athletes_csv_multithreaded(input_file, output_file, name_column='name', \n",
    "                                      max_workers=10, limit=None):\n",
    "    \"\"\"Process the full CSV with multithreading.\"\"\"\n",
    "    logger.info(f\"Reading CSV file: {input_file}\")\n",
    "    \n",
    "    try:\n",
    "        df = pd.read_csv(input_file)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error reading CSV: {e}\")\n",
    "        return\n",
    "    \n",
    "    if name_column not in df.columns:\n",
    "        logger.error(f\"Column '{name_column}' not found. Available: {df.columns.tolist()}\")\n",
    "        return\n",
    "    \n",
    "    if 'image_url' not in df.columns:\n",
    "        df['image_url'] = None\n",
    "    \n",
    "    if limit:\n",
    "        df_process = df.head(limit).copy()\n",
    "    else:\n",
    "        df_process = df.copy()\n",
    "    \n",
    "    total_rows = len(df_process)\n",
    "    logger.info(f\"Processing {total_rows} athletes with {max_workers} threads...\")\n",
    "    \n",
    "    counter = ProgressCounter(total_rows)\n",
    "    \n",
    "    args_list = [\n",
    "        (idx, row[name_column], counter)\n",
    "        for idx, row in df_process.iterrows()\n",
    "        if pd.isna(row['image_url'])\n",
    "    ]\n",
    "    \n",
    "    if not args_list:\n",
    "        logger.info(\"All athletes already have image URLs!\")\n",
    "        return\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = {executor.submit(scrape_single_athlete, args): args for args in args_list}\n",
    "        \n",
    "        for future in as_completed(futures):\n",
    "            try:\n",
    "                idx, image_url = future.result()\n",
    "                df.loc[idx, 'image_url'] = image_url\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Thread execution error: {e}\")\n",
    "    \n",
    "    elapsed_time = time.time() - start_time\n",
    "    \n",
    "    logger.info(f\"Saving results to: {output_file}\")\n",
    "    df.to_csv(output_file, index=False)\n",
    "    \n",
    "    total = len(df_process)\n",
    "    with_images = df_process['image_url'].notna().sum()\n",
    "    \n",
    "    logger.info(f\"\\n{'='*60}\")\n",
    "    logger.info(f\"FINAL SUMMARY:\")\n",
    "    logger.info(f\"Total athletes: {total}\")\n",
    "    logger.info(f\"Images found: {with_images} ({with_images/total*100:.1f}%)\")\n",
    "    logger.info(f\"Images missing: {total-with_images} ({(total-with_images)/total*100:.1f}%)\")\n",
    "    logger.info(f\"Time elapsed: {elapsed_time:.2f} seconds\")\n",
    "    logger.info(f\"Average time per athlete: {elapsed_time/total:.2f} seconds\")\n",
    "    logger.info(f\"{'='*60}\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2332acbb-f64c-43f3-9e2e-be1fe54ac319",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "    INPUT_CSV = \"athletes.csv\"\n",
    "    NAME_COLUMN = \"name\"\n",
    "    \n",
    "    # Option 1: Debug a single athlete to see what's happening\n",
    "    print(\"\\nDEBUG MODE: Testing single athlete\")\n",
    "    print(\"=\"*60)\n",
    "    debug_single_athlete(\"ALEKSANYAN Artur\", save_html=True)\n",
    "    \n",
    "    # Option 2: Run test on first 10 rows\n",
    "    print(\"\\n\\nTEST MODE: Testing first 10 rows\")\n",
    "    print(\"=\"*60)\n",
    "    test_results = test_first_n_rows(\n",
    "        input_file=INPUT_CSV,\n",
    "        name_column=NAME_COLUMN,\n",
    "        n=10,\n",
    "        max_workers=10\n",
    "    )\n",
    "    \n",
    "    if test_results is not None and test_results['image_url'].notna().any():\n",
    "        proceed = input(\"\\n✓ Some images found! Process full CSV? (yes/no): \").strip().lower()\n",
    "        if proceed == 'yes':\n",
    "            OUTPUT_CSV = \"athletes_with_images.csv\"\n",
    "            process_athletes_csv_multithreaded(\n",
    "                input_file=INPUT_CSV,\n",
    "                output_file=OUTPUT_CSV,\n",
    "                name_column=NAME_COLUMN,\n",
    "                max_workers=10,\n",
    "                limit=None\n",
    "            )\n",
    "    else:\n",
    "        print(\"\\n✗ No images found. Check the debug output and saved HTML file.\")\n",
    "        print(\"The website structure may have changed or athletes may not have pages.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
